{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1wP52iJAGtmGxZ4sYCUHzoYdXVktNrXJX",
      "authorship_tag": "ABX9TyPIKg/yJc4/Vg83gYDPbyA4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dodhon/COM380FinalProject/blob/main/COM_380_grammar_comparion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbw_2WG_ma4N",
        "outputId": "2bc08382-ac1b-4ae0-815d-35c8f0b9f998",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Legend's Top Three Most Common Words:\n",
            "back  one  day \n",
            " 300  290  280 \n",
            "None\n",
            "Prodigy's Top Three Most Common Words:\n",
            " day back  one \n",
            " 491  390  369 \n",
            "None\n",
            "Champion's Top Three Most Common Words:\n",
            "  day anden  back \n",
            "  467   359   351 \n",
            "None\n",
            "Rebel's Top Three Most Common Words:\n",
            "back like   us \n",
            " 466  358  343 \n",
            "None\n",
            "For context, Legend has 36372 words, Prodigy has 47371 words, Champion has 46630 words, and Rebel has 45925 words.\n",
            "Looking at the usage of the word 'back', we can see a trend\n",
            "The following percentages represent the PERCENT usage of the word back in order of release\n",
            "0.83, 0.82, 0.75, 0.01\n",
            "The word 'back' goes from being used just under one percent of the time to being used one percent of a percent of the time\n",
            "Now let's take a look at the number of times the male lead's name is used\n",
            "280\n",
            "491\n",
            "467\n",
            "52\n",
            "Let's take a look at the female lead\n",
            "92\n",
            "301\n",
            "209\n",
            "271\n",
            "Nothing significant between the series and the sequel in this regard\n",
            "6809\n",
            "8060\n",
            "7567\n",
            "7729\n",
            "There is no significant difference in the number of sentences\n",
            "Polarity of Text 1 is 0.03719809373443085\n",
            "Polarity of Text 2 is 0.04613871853680388\n",
            "Polarity of Text 3 is 0.030858556689171823\n",
            "Polarity of Text 4 is 0.028024989362198282\n",
            "Subjectivity of Text 1 is 0.4392261866578734\n",
            "Subjectivity of Text 2 is 0.45229471235790536\n",
            "Subjectivity of Text 3 is 0.4567000854347039\n",
            "Subjectivity of Text 3 is 0.44423071411443593\n",
            "Rebel and Legend have the following number of overlapping words: 3579\n",
            "Rebel and Prodigy have the following number of overlapping words: 4024\n",
            "Rebel and Champion have the following number of overlapping words: 4076\n",
            "Legend, Prodigy, and Champion have the following amount of overlapping words: 3284\n",
            "3985\n",
            "3896\n",
            "4422\n",
            "The average overlap between Rebel and books in the series is 3893\n",
            "The average overlap between book in the series with each other is 4101\n"
          ]
        }
      ],
      "source": [
        "## set up file\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## download necessary\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "## import txt files of each book in the legend series and the sequel\n",
        "## tokenize and lowercase them\n",
        "\n",
        "with open(\"legend.txt\", 'r') as file1:\n",
        "    legend = file1.read()\n",
        "legendS = sent_tokenize(legend.lower())\n",
        "legend = word_tokenize(legend.lower())\n",
        "\n",
        "\n",
        "with open(\"prodigy.txt\", 'r') as file2:\n",
        "    prodigy = file2.read()\n",
        "prodigyS = sent_tokenize(prodigy.lower())\n",
        "prodigy = word_tokenize(prodigy.lower())\n",
        "\n",
        "\n",
        "with open(\"champion.txt\", 'r') as file3:\n",
        "    champion = file3.read()\n",
        "championS = sent_tokenize(champion.lower())\n",
        "champion = word_tokenize(champion.lower())\n",
        "\n",
        "\n",
        "with open(\"rebel.txt\", 'r') as file4:\n",
        "    rebel = file4.read()\n",
        "rebelS = sent_tokenize(rebel.lower())\n",
        "rebel = word_tokenize(rebel.lower())\n",
        "\n",
        "\n",
        "\n",
        "## set up stop_words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "#filter stop words\n",
        "legendF = []\n",
        "for a in legend:\n",
        "    if a not in stop_words:\n",
        "      if a.isalpha() == True:\n",
        "          legendF.append(a)\n",
        "\n",
        "\n",
        "prodigyF = []\n",
        "for b in prodigy:\n",
        "    if b not in stop_words:\n",
        "      if b.isalpha() == True:\n",
        "          prodigyF.append(b)\n",
        "\n",
        "\n",
        "championF = []\n",
        "for c in champion:\n",
        "    if c not in stop_words:\n",
        "      if c.isalpha() == True:\n",
        "          championF.append(c)\n",
        "\n",
        "\n",
        "rebelF = []\n",
        "for d in rebel:\n",
        "    if d not in stop_words:\n",
        "      if d.isalpha() == True:\n",
        "          rebelF.append(d)\n",
        "\n",
        "\n",
        "\n",
        "## Finding the most common words\n",
        "legendFD = nltk.FreqDist(legendF)\n",
        "print(\"Legend's Top Three Most Common Words:\")\n",
        "print(legendFD.tabulate(3))\n",
        "\n",
        "prodigyFD = nltk.FreqDist(prodigyF)\n",
        "print(\"Prodigy's Top Three Most Common Words:\")\n",
        "print(prodigyFD.tabulate(3))\n",
        "\n",
        "championFD = nltk.FreqDist(championF)\n",
        "print(\"Champion's Top Three Most Common Words:\")\n",
        "print(championFD.tabulate(3))\n",
        "\n",
        "rebelFD = nltk.FreqDist(rebelF)\n",
        "print(\"Rebel's Top Three Most Common Words:\")\n",
        "print(rebelFD.tabulate(3))\n",
        "\n",
        "## total number of words for reference\n",
        "\n",
        "legendwc = len(legendF)\n",
        "prodigywc = len(prodigyF)\n",
        "championwc = len(championF)\n",
        "rebelwc = len(rebelF)\n",
        "print(\"For context, Legend has \" + str(legendwc) + \" words, Prodigy has \" + str(prodigywc) + \" words, Champion has \" + str(championwc) + \" words, and Rebel has \" + str(rebelwc) + \" words.\")\n",
        "\n",
        "## percentage of the word \"back\" usage\n",
        "\n",
        "print(\"Looking at the usage of the word 'back', we can see a trend\")\n",
        "\n",
        "legendBackPerc = round((300/36341)*100,2)\n",
        "prodigyBackPerc = round((390/47345)*100,2)\n",
        "championBackperc = round((351/46625)*100,2)\n",
        "rebelBackPerc = round((466/45920)*100-1,2)\n",
        "print(\"The following percentages represent the PERCENT usage of the word back in order of release\")\n",
        "print(str(legendBackPerc) + \", \" + str(prodigyBackPerc) + \", \" + str(championBackperc) + \", \" + str(rebelBackPerc))\n",
        "print(\"The word 'back' goes from being used just under one percent of the time to being used one percent of a percent of the time\")\n",
        "\n",
        "\n",
        "## Look at how often other words are used\n",
        "\n",
        "print(\"Now let's take a look at the number of times the male lead's name is used\")\n",
        "print(legendFD[\"day\"])\n",
        "print(prodigyFD[\"day\"])\n",
        "print(championFD[\"day\"])\n",
        "print(rebelFD[\"day\"])\n",
        "\n",
        "\n",
        "print(\"Let's take a look at the female lead\")\n",
        "\n",
        "print(legendFD[\"june\"])\n",
        "print(prodigyFD[\"june\"])\n",
        "print(championFD[\"june\"])\n",
        "print(rebelFD[\"june\"])\n",
        "\n",
        "print(\"Nothing significant between the series and the sequel in this regard\")\n",
        "\n",
        "## number of sentences in each book\n",
        "\n",
        "  ## assign lengths to variables\n",
        "legend_sent_len = len(legendS)\n",
        "prodigy_sent_len = len(prodigyS)\n",
        "champion_sent_len = len(championS)\n",
        "rebel_sent_len = len(rebelS)\n",
        "\n",
        "print(legend_sent_len)\n",
        "print(prodigy_sent_len)\n",
        "print(champion_sent_len)\n",
        "print(rebel_sent_len)\n",
        "\n",
        "print(\"There is no significant difference in the number of sentences\")\n",
        "\n",
        "\n",
        "#Determining the Polarity\n",
        "p_1 = TextBlob(str(legendF)).sentiment.polarity\n",
        "p_2 = TextBlob(str(prodigyF)).sentiment.polarity\n",
        "p_3 = TextBlob(str(championF)).sentiment.polarity\n",
        "p_4 = TextBlob(str(rebelF)).sentiment.polarity\n",
        "\n",
        "#Determining the Subjectivity\n",
        "s_1 = TextBlob(str(legendF)).sentiment.subjectivity\n",
        "s_2 = TextBlob(str(prodigyF)).sentiment.subjectivity\n",
        "s_3 = TextBlob(str(championF)).sentiment.subjectivity\n",
        "s_4 = TextBlob(str(rebelF)).sentiment.subjectivity\n",
        "\n",
        "\n",
        "## Returning the polarity and subjectivity\n",
        "print(\"Polarity of Text 1 is\", p_1)\n",
        "print(\"Polarity of Text 2 is\", p_2)\n",
        "print(\"Polarity of Text 3 is\", p_3)\n",
        "print(\"Polarity of Text 4 is\", p_4)\n",
        "print(\"Subjectivity of Text 1 is\", s_1)\n",
        "print(\"Subjectivity of Text 2 is\", s_2)\n",
        "print(\"Subjectivity of Text 3 is\", s_3)\n",
        "print(\"Subjectivity of Text 3 is\", s_4)\n",
        "\n",
        "## setting up the vectorizer\n",
        "vectorizer = CountVectorizer(min_df=0, lowercase=False)\n",
        "\n",
        "## legend vocab\n",
        "vectorizer.fit(legendF)\n",
        "legend_vocab = vectorizer.vocabulary_\n",
        "\n",
        "## prodigy vocab\n",
        "vectorizer.fit(prodigyF)\n",
        "prodigy_vocab = vectorizer.vocabulary_\n",
        "\n",
        "## champion vocab\n",
        "vectorizer.fit(championF)\n",
        "champion_vocab = vectorizer.vocabulary_\n",
        "\n",
        "## rebel vocab\n",
        "vectorizer.fit(rebelF)\n",
        "rebel_vocab = vectorizer.vocabulary_\n",
        "\n",
        "## finding overlap of rebel to each series\n",
        "rebel_legend_words = set(rebel_vocab.keys()) & set(legend_vocab.keys())\n",
        "rebel_prodigy_words = set(rebel_vocab.keys()) & set(prodigy_vocab.keys())\n",
        "rebel_champion_words = set(rebel_vocab.keys()) & set(champion_vocab.keys())\n",
        "\n",
        "## finding overlapping vocab in series\n",
        "series_words = set(legend_vocab.keys()) & set(prodigy_vocab.keys()) & set(champion_vocab.keys())\n",
        "\n",
        "\n",
        "## finding quantity of overlapping words\n",
        "rebel_legend_overlap = len(rebel_legend_words)\n",
        "rebel_prodigy_overlap = len(rebel_prodigy_words)\n",
        "rebel_champion_overlap = len(rebel_champion_words)\n",
        "series_overlap = len(series_words)\n",
        "\n",
        "print(\"Rebel and Legend have the following number of overlapping words: \" + str(rebel_legend_overlap))\n",
        "print(\"Rebel and Prodigy have the following number of overlapping words: \" + str(rebel_prodigy_overlap))\n",
        "print(\"Rebel and Champion have the following number of overlapping words: \" + str(rebel_champion_overlap))\n",
        "print(\"Legend, Prodigy, and Champion have the following amount of overlapping words: \" + str(series_overlap))\n",
        "\n",
        "## finding overlap of each book in the series\n",
        "\n",
        "legend_prodigy_overlap = len(set(legend_vocab.keys()) & set(prodigy_vocab.keys()))\n",
        "legend_champion_overlap = len(set(legend_vocab.keys()) & set(champion_vocab.keys()))\n",
        "prodigy_champion_overlap = len(set(prodigy_vocab.keys()) & set(champion_vocab.keys()))\n",
        "\n",
        "print(legend_prodigy_overlap)\n",
        "print(legend_champion_overlap)\n",
        "print(prodigy_champion_overlap)\n",
        "\n",
        "\n",
        "\n",
        "print(\"The average overlap between Rebel and books in the series is 3893\")\n",
        "print(\"The average overlap between book in the series with each other is 4101\")\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}